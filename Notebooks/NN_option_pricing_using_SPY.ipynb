{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeeeedb4",
   "metadata": {},
   "source": [
    "## Neural Net Option Pricing on SPY Data \n",
    "We test the model on the SPY Options OPRA data \n",
    "- Check the EAD on SPY Data on `SPY_DATA.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bffc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, qmc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yfinance\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6421fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b30af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11f2c4310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24948433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.261616e+06\n",
       "mean     1.802652e+02\n",
       "std      2.348329e+02\n",
       "min      1.000000e+00\n",
       "25%      1.500000e+01\n",
       "50%      8.000000e+01\n",
       "75%      2.520000e+02\n",
       "max      1.078000e+03\n",
       "Name: dte, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_market = pd.read_csv(\"/Users/karandeepsonewane/Projects/Option Pricing using Neural Network/data/spy_option/processed/option_chain_with_vix.csv\")\n",
    "df = df_market.copy()\n",
    "df = df.drop('VIX', axis=1)\n",
    "df = df.rename(columns={\"sigma\" : \"VIX\"})\n",
    "dte_data = df[\"dte\"].describe()\n",
    "dte_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380e1f5",
   "metadata": {},
   "source": [
    "#### Loading, Cleaning and Feature Engineering\n",
    "- Fetaure\n",
    "    - `M`= log(S/K): Log moneyness \n",
    "    - `t_ann` = time to maturity annulized \n",
    "    - `sigma` = VIX is chosen as a proxy, if VIX is not available then constant Vol = 0.20 \n",
    "      - IV is avoided to avoid circularity  \n",
    "- Traget \n",
    "    - `y_{c/k}`= `mid`/`Strike` (Normalized Call price by Strike price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42d70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required columns exist\n",
    "req = [\"snapshot_date\", \"t_ann\", \"M\", \"mid\", \"strike\"]\n",
    "\n",
    "# parsing date \n",
    "df[\"snapshot_date\"] = pd.to_datetime(df[\"snapshot_date\"], errors=\"coerce\")\n",
    "df = df[df[\"snapshot_date\"].notna()].copy()\n",
    "\n",
    "# Clean data\n",
    "df = df[np.isfinite(df[[\"t_ann\",\"M\",\"mid\",\"strike\"]]).all(axis=1)].copy()\n",
    "df = df[(df[\"t_ann\"] > 0) & (df[\"strike\"] > 0) & (df[\"mid\"] >= 0)].copy()\n",
    "\n",
    "# Create target: C/K\n",
    "df[\"y_ck\"] = (df[\"mid\"] / df[\"strike\"]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdcea50",
   "metadata": {},
   "source": [
    "##### Data Check\n",
    "We enforce basic financial and numerical sanity checks prior to model training. Time-to-maturity (t_ann) is required to be strictly positive to avoid degenerate contracts. All model inputs (M, t_ann, VIX) are constrained to be finite, excluding missing or infinite values. Option strikes are restricted to positive values, and observed mid-prices are constrained to be non-negative, consistent with no-arbitrage bounds. These assertions ensure numerical stability and economic validity of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770d3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert \n",
    "assert (df_market[\"t_ann\"] > 0).all()\n",
    "assert np.isfinite(df_market[[\"M\", \"t_ann\", \"sigma\"]]).all().all(), \"NaNs or infs in X\"\n",
    "assert (df_market[\"strike\"] > 0).all(), \"Invalid strikes\"\n",
    "assert (df_market[\"mid\"] >= 0).all(), \"Negative prices\"            \n",
    "  \n",
    "# target variable \n",
    "df_market[\"C/K\"] = df_market[\"mid\"] / df_market[\"strike\"]\n",
    "df_market[\"C_dollar\"] = df_market[\"mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69280658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sigma(df):\n",
    "    if \"VIX\" in df.columns and np.isfinite(df[\"VIX\"]).mean() > 0.99:\n",
    "        return \"VIX\"\n",
    "    df[\"sigma_proxy\"] = 0.20\n",
    "    return \"sigma_proxy\"\n",
    "\n",
    "sigma_col = pick_sigma(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def32ff",
   "metadata": {},
   "source": [
    "#### Train:Test Split: Time based split \n",
    "- sorted by dates \n",
    "- first 80% dates for training \n",
    "- last 20% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601cdd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique dates: 272 | Train dates: 217 | Test dates: 55\n",
      "Train rows: 980328 | Test rows: 281288\n"
     ]
    }
   ],
   "source": [
    "dates = np.array(sorted(df[\"snapshot_date\"].unique()))\n",
    "if len(dates) < 5:\n",
    "    raise ValueError(f\"Need more unique dates for time split, got {len(dates)}\")\n",
    "\n",
    "\n",
    "# 80:20\n",
    "split = int(0.8 * len(dates))\n",
    "train_dates = set(dates[:split])\n",
    "test_dates  = set(dates[split:])\n",
    "\n",
    "\n",
    "# split base on time \n",
    "train_df = df[df[\"snapshot_date\"].isin(train_dates)].copy()\n",
    "test_df  = df[df[\"snapshot_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Unique dates: {len(dates)} | Train dates: {len(train_dates)} | Test dates: {len(test_dates)}\")\n",
    "print(f\"Train rows: {len(train_df)} | Test rows: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a28a36",
   "metadata": {},
   "source": [
    "#### Gaussian weighted loss function calcualtion for the real data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfe6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weights implementation\n",
    "# def gaussian_weights(df, lam=1, beta=0.5, M_col=\"M\", T_col=\"t_ann\", normalize_mean=True):\n",
    "#     M = df[M_col].astype(float).to_numpy()\n",
    "#     T = df[T_col].astype(float).to_numpy()\n",
    "    \n",
    "#     # weights\n",
    "#     w = np.exp(-(M**2) / (2.0 * lam**2) - beta * T).astype(np.float32)\n",
    "#     w = np.clip(w, 1e-4, None)        #  prevents zero-weight underflow\n",
    "#     w = w / (np.mean(w) + 1e-8)\n",
    "    \n",
    "#     # Normalize weights to mean=1\n",
    "#     if normalize_mean:\n",
    "#         w = w / (np.mean(w) + 1e-8)\n",
    "    \n",
    "#     return w\n",
    "\n",
    "def gaussian_weights(\n",
    "    df,\n",
    "    lam=0.4,              # wider ATM band\n",
    "    beta=5.0,             # much softer time decay\n",
    "    M_col=\"M\",\n",
    "    T_col=\"t_ann\",\n",
    "    normalize_mean=True,\n",
    "    w_floor_itm=0.05      # << NEW\n",
    "):\n",
    "    M = df[M_col].to_numpy(float)\n",
    "    T = df[T_col].to_numpy(float)\n",
    "\n",
    "    w = np.exp(-(M**2)/(2*lam**2) - beta*T)\n",
    "\n",
    "    # region-aware floor\n",
    "    itM = M > 0.05\n",
    "    w[itM] = np.maximum(w[itM], w_floor_itm)\n",
    "\n",
    "    w = w.astype(np.float32)\n",
    "    w /= (np.mean(w) + 1e-8)\n",
    "\n",
    "    if normalize_mean:\n",
    "        w /= (np.mean(w) + 1e-8)\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51e8c1",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "GWMSE Model \n",
    "- Lambda and Beta \n",
    "MiX Model \n",
    "- ALPHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549babfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train weights: min=5.7255e-07, max=2.5315e+00, mean=1.0000, median=0.7238\n"
     ]
    }
   ],
   "source": [
    "LAMBDA = 0.4   # lambda (log-moneyness width) \n",
    "BETA   = 5.0   # beta (time penalty rate)\n",
    "ALPHA = 0.5\n",
    "\n",
    "\n",
    "w_train = gaussian_weights(train_df, \n",
    "                           lam=LAMBDA, \n",
    "                           beta=BETA)\n",
    "w_test  = gaussian_weights(test_df,  \n",
    "                           lam=LAMBDA, \n",
    "                           beta=BETA)\n",
    "\n",
    "\n",
    "# chceck tarin weights\n",
    "print(f\"Train weights: min={w_train.min():.4e}, max={w_train.max():.4e}, mean={w_train.mean():.4f}, median={np.median(w_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172f4b4",
   "metadata": {},
   "source": [
    "#### Feature Engineering \n",
    "#### Z-Score Standardization\n",
    "The scale and distribution of the inputs are heterogeous and hence required standardization for numerical stability and gradient scaling. Therefore Z-score scaling has been used with the following properties:\n",
    "- compute the mu and sigma on training data, and freeze it to avoid lookahead bias \n",
    "- apply the same transformation on the test data \n",
    "$$\\hat x_{test} = \\frac{x_{test} - \\mu_{train}}{\\sigma_{train}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c9e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"t_ann\", \"M\", sigma_col]  # X=[T, logM, sigma]\n",
    "\n",
    "# Input features \n",
    "X_train = train_df[FEATURES].to_numpy(np.float32)\n",
    "X_test  = test_df[FEATURES].to_numpy(np.float32)\n",
    "\n",
    "# target variables \n",
    "y_train = train_df[\"y_ck\"].to_numpy(np.float32)                          \n",
    "y_test  = test_df[\"y_ck\"].to_numpy(np.float32)\n",
    "\n",
    "# Compute mean/std on TRAIN only\n",
    "mu = X_train.mean(axis=0)                                             \n",
    "sd = X_train.std(axis=0)\n",
    "sd = np.where(sd < 1e-8, 1.0, sd)\n",
    "\n",
    "# Apply to both train and test \n",
    "# the mu and sigma are calulated on the training set only to avoid lookahead bias \n",
    "X_train_z = (X_train - mu) / sd\n",
    "X_test_z  = (X_test  - mu) / sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a37208",
   "metadata": {},
   "source": [
    "#### DataLoader with weights \n",
    "-  batching the dataset into batches for efficient trainig \n",
    "- using dataloader for bathcing \n",
    "- shuffle=True, training will be shuffled but (only on the training dataset)\n",
    "\n",
    "PyTorch `DataLoader` is a utility class designed to simplify the load and iteration over the datset for training. It has various contraints for the batching, suffling and processing data. \n",
    "- Batching helps to reduce the memory footprint by leveraging parallel processing\n",
    "- shuffling prevents the model to overfit to noise by shuffling the dataset on the learning, provides more stable training and avoid stucking in local minima \n",
    "\n",
    "In our code:\n",
    "we are loading the whole dataset from the disk into ram (since dataset is small), converting it into tensor (it lives on RAM), then we `__getitem__` lazy, from the ram on the index i (all labels at i), this avoid making a copy on the ram for the shuffled data on the RAM. Which is abstracted by the the DataLoader class by pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b7a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data loading and batching \n",
    "class OptionDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        # convert into tensor \n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.w = torch.tensor(w, dtype=torch.float32)  # Weights \n",
    "    \n",
    "    # length of data    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # get each values at index i\n",
    "    def __getitem__(self, i):                                         \n",
    "        return self.X[i], self.y[i], self.w[i]\n",
    "    \n",
    "\n",
    "# batch training \n",
    "BATCH = 2048\n",
    "\n",
    "train_ds = OptionDataset(X_train_z, y_train, w_train)\n",
    "test_ds  = OptionDataset(X_test_z,  y_test,  w_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=False)        \n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29d4f9",
   "metadata": {},
   "source": [
    "#### Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14be5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5K param model same as above \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden=(64, 64), dropout=0.0, use_softplus=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(d, h))\n",
    "            layers.append(nn.ELU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            d = h  # update input dimension for next layer\n",
    "\n",
    "        self.body = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(d, 1)  # d is last hidden size\n",
    "        self.softplus = nn.Softplus() if use_softplus else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.out(x)\n",
    "        x = self.softplus(x)        # enforces non-negative output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739a85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 1])\n"
     ]
    }
   ],
   "source": [
    "m = MLP(in_dim=3, hidden=(64,64), use_softplus=True)\n",
    "xb = torch.randn(2048, 3)\n",
    "print(m(xb).shape)  # should be torch.Size([2048, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099347e",
   "metadata": {},
   "source": [
    "##### Loss function design\n",
    "- we are using the same example as we used in the synthetic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04fd212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(pred, y):\n",
    "    return torch.mean((pred - y) ** 2)\n",
    "\n",
    "def loss_wmse(pred, y, w):\n",
    "    # wighted mse formula see equation*\n",
    "    return torch.sum(w * (pred - y) ** 2) / torch.sum(w)\n",
    "\n",
    "# Mixture Loss:  alpha*WMSE + (1-alpha)*MSE\n",
    "def loss_mix(pred, y, w, alpha=0.9):\n",
    "    mse  = torch.mean((pred - y) ** 2)\n",
    "    wmse = torch.sum(w * (pred - y) ** 2) / (torch.sum(w) + 1e-8)\n",
    "    return alpha * wmse + (1.0 - alpha) * mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444beda",
   "metadata": {},
   "source": [
    "#### Training and Testing\n",
    "- Outer Split: Split the data into 80:20 (train:test)\n",
    "- inner split: Split train data into 90:10 (train:val) \n",
    "- keep test data completely out of sample \n",
    "- validation use early stopping and model selection without peeking the test \n",
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052ae1b",
   "metadata": {},
   "source": [
    "##### Helper functions \n",
    "1. inner split \n",
    "2. mask for inner split and validation \n",
    "3. feature selection and numpy extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b4cbc",
   "metadata": {},
   "source": [
    "#### Temporal Data Partitioning \n",
    "`inner_split_masks`: This function creates a temporal split on the data by sorting the date and hold out the future date for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c091b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_split_masks(train_df_,\n",
    "                            date_col = \"snapshot_date\", \n",
    "                            frac_train=0.9\n",
    "                            ):\n",
    "    \"\"\"_summary_\n",
    "    Time-based split unique dates in the date column,\n",
    "    prevents look ahead bias by construction \n",
    "\n",
    "    Returns:\n",
    "        inner_mask and val_mask for the time based split for the training dataset \n",
    "    \"\"\"\n",
    "    # assert missing date col\n",
    "    assert date_col in train_df_.columns, \"Missing Date Column\"\n",
    "    \n",
    "    # arrange chronologically and select the dates \n",
    "    dts = np.array(sorted(train_df_[date_col].unique()))\n",
    "    \n",
    "    # cut select the index max of training data, which we will use later for validation, \n",
    "    # this avoid look-ahead bias in the validation set \n",
    "    cut = max(1, int(frac_train * len(dts)))\n",
    "    \n",
    "    # return set for past and future dts for immutable faster lookup later \n",
    "    inner_train_dates = set(dts[:cut])\n",
    "    val_dates = set(dts[cut:])\n",
    "    \n",
    "    # convert it into inner and val mask  \n",
    "    inner_mask = train_df_[date_col].isin(inner_train_dates).to_numpy()\n",
    "    val_mask   = train_df_[date_col].isin(val_dates).to_numpy()\n",
    "\n",
    "    # safe assertion \n",
    "    assert inner_mask.shape == val_mask.shape                                                 # shape check \n",
    "    assert inner_mask.sum() > 0, \"No rows in inner train split\"                               # greater than 0 rows\n",
    "    assert val_mask.sum() > 0, \"No rows in val split\"                                       \n",
    "    assert not np.any(inner_mask & val_mask), \"Overlap between inner train and val!\"          # no overlap for look ahead bias in val\n",
    "\n",
    "    return inner_mask, val_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f608c",
   "metadata": {},
   "source": [
    "#### Data preperation and Data Integrity \n",
    "- `extract_arrays`: extracts the feature and target columns and convert them into np.float32 for later use \n",
    "- `fit_zscore`: Explicitly calls the fit_zscore(X_inner) only on the training set and hence avoid *Data leakage* from the test dataset. For numerical robustness np.where(sd < eps) ensures that the there is 1 SD instead of NaN or Zero  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62679ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Extraction and Numpy conversion\n",
    "def extract_arrays(df_, \n",
    "                   feature_cols,\n",
    "                   y_col=\"y_ck\", \n",
    "                   dtype=np.float32):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df_: training dataFrame \n",
    "        feature_cols: t_ann, M and sigma  \n",
    "        y_col: Strike normalized Call price \n",
    "        dtype: Defaults to np.float32.\n",
    "\n",
    "    Returns:\n",
    "        X, y numpy arrays \n",
    "    \"\"\"\n",
    "\n",
    "# check if column exists in df\n",
    "    assert all(c in df_.columns for c in feature_cols), f\"Missing features: {feature_cols}\"  \n",
    "    assert y_col in df_.columns, f\"Missing target col: {y_col}\"  \n",
    "\n",
    "    X = df_[feature_cols].to_numpy(dtype)\n",
    "    y = df_[y_col].to_numpy(dtype)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# apply z-score for inner traing only \n",
    "def fit_zscore(X, eps=1e-8):   \n",
    "    mu = X.mean(axis=0)\n",
    "    sd = X.std(axis=0)\n",
    "    sd = np.where(sd < eps, 1.0, sd)     # if feature has zero variance -> sd = 0 -> default to 1   \n",
    "    return mu, sd\n",
    "\n",
    "def apply_zscore(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "\n",
    "# inner val loader and tensors \n",
    "def make_inner_val_loaders(\n",
    "    train_df_,\n",
    "    feature_cols,\n",
    "    w_train_all,                 # weights aligned with train_df_ rows (same order)\n",
    "    date_col=\"snapshot_date\",\n",
    "    frac_train=0.9,\n",
    "    batch_size=2048  \n",
    "):\n",
    "    \n",
    "    # masks\n",
    "    inner_mask, val_mask = inner_split_masks(train_df_, \n",
    "                                                   date_col=date_col, \n",
    "                                                   frac_train=frac_train)\n",
    "\n",
    "    # arrays\n",
    "    X_all, y_all = extract_arrays(train_df_, \n",
    "                                  feature_cols, \n",
    "                                  y_col=\"y_ck\")\n",
    "\n",
    "    # split\n",
    "    X_inner, y_inner, w_inner = X_all[inner_mask], y_all[inner_mask], w_train_all[inner_mask]\n",
    "    X_val,   y_val,   w_val   = X_all[val_mask],   y_all[val_mask],   w_train_all[val_mask]\n",
    "\n",
    "    # fit scaler on inner-train only - no lookahead into val  \n",
    "    mu, sd = fit_zscore(X_inner)\n",
    "\n",
    "    # the X_val and X_inner z score values are kept seperate \n",
    "    X_inner_z = apply_zscore(X_inner, mu, sd).astype(np.float32)       \n",
    "    X_val_z   = apply_zscore(X_val,   mu, sd).astype(np.float32) \n",
    "\n",
    "    # generator seed\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    \n",
    "    # datasets/loaders\n",
    "    inner_ds = OptionDataset(X_inner_z, y_inner, w_inner)\n",
    "    inner_loader = DataLoader(inner_ds, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              drop_last=False, \n",
    "                              generator=g)\n",
    "\n",
    "    # val tensors\n",
    "    X_val_t = torch.tensor(X_val_z, dtype=torch.float32, device=DEVICE)\n",
    "    y_val_t = torch.tensor(y_val,   dtype=torch.float32, device=DEVICE)\n",
    "    w_val_t = torch.tensor(w_val,   dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    return inner_loader, (X_val_t, y_val_t, w_val_t), (mu, sd), (inner_mask, val_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b3d136",
   "metadata": {},
   "source": [
    "##### Training function for EPOCH and Loss Calcualtions\n",
    "This function implements a single epoch training using opt=ADAM optimizer. During each iteration, loader from DataLoader provides the mini-batches of inputs (xb), targets (yb) and sample weights (wb). All tensor are passed to DEVICE. \n",
    "\n",
    "The objective is selected dynamically using the mode parameter:\n",
    "1. 'mse': Mean squared error, treating all the error samples uniformly \n",
    "2. 'wmse': wighted mean square error with the ephasis on ATM-short dated options\n",
    "3. 'mix': a convex combination of MSE and WMSE, controlled by a mixing coefficient alpha\n",
    "\n",
    "Before backprop, gradients are cleared, setting graients to None. Epoch level loss is accumulated as a **sum of batch loss weighted by batch size** rather than averaging per-batch loss directly. This ensure partial batches are handled correctly and loss represent the true sample level avg loss across epochs. \n",
    "\n",
    "The function returns the avergae loss per observation for the epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# training one EPOCH  \n",
    "def train_one_epoch(model, loader, opt, mode=\"mse\"): \n",
    "    model.train()                                          # set model to training            \n",
    "    total_loss = 0.0                                       # total loss \n",
    "    n_obs = 0                                              # total no of observation in the batch\n",
    "\n",
    "    for xb, yb, wb in loader:                              # chuck training from loader \n",
    "        xb = xb.to(DEVICE)                                     # laoder handles the shuffling of batch data         \n",
    "        yb = yb.to(DEVICE)                                        # handles partial batches \n",
    "        wb = wb.to(DEVICE)\n",
    "\n",
    "        pred = model(xb).squeeze(-1)\n",
    "\n",
    "        if mode == \"mse\":                                     \n",
    "            loss = loss_mse(pred, yb)\n",
    "        elif mode == \"wmse\":\n",
    "            loss = loss_wmse(pred, yb, wb)\n",
    "        elif mode == \"mix\":\n",
    "            loss = loss_mix(pred, yb, wb, alpha=ALPHA)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)                       # Optimization \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        bs = xb.shape[0]                                      # calcualte the batch size              \n",
    "        total_loss += float(loss.item()) * bs                 # total loss = loss * batch size; convert tensor into -> float \n",
    "        n_obs += bs\n",
    "\n",
    "    return total_loss / max(1, n_obs)                          # final loss = total loss / avg sample in epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416964ca",
   "metadata": {},
   "source": [
    "##### Loss Evaluation: MSE, WMSE and MIX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b68c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function for both methods \n",
    "@torch.no_grad()                                       # no grad for eval \n",
    "def eval_loss(model, \n",
    "              X_t, \n",
    "              y_t, \n",
    "              w_t, \n",
    "              mode=\"mse\"):\n",
    "    model.eval()\n",
    "    pred = model(X_t).squeeze(-1)\n",
    "\n",
    "    if mode == \"mse\":                                  # two models for comparision with different loss function\n",
    "        return float(loss_mse(pred, y_t).item())\n",
    "    elif mode == \"wmse\":                              \n",
    "        return float(loss_wmse(pred, y_t, w_t).item())\n",
    "    elif mode == \"mix\":\n",
    "        return float(loss_mix(pred, y_t, w_t, alpha=ALPHA).item())\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba6e33",
   "metadata": {},
   "source": [
    "##### Model Training and Early Stopping \n",
    "- this function uses the `model`, `inner_loader` (to load the batched data), `tain_one_epoch`, and `eval_loss`\n",
    "- `model_train_ES` function loop over multiple EPOCH to train the model with early stopping, the early stopping condition is checked using the validation loss with a small tolerance to create a buffer for improvement in the model, the best model weights are then saved and loaded using the `state_dict` object (python dict), it returns model and best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d60150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with early stopping \n",
    "def model_train_ES(\n",
    "    model,\n",
    "    inner_loader,\n",
    "    val_pack,                 # (X_val_t, y_val_t, w_val_t)\n",
    "    mode=\"mse\",\n",
    "    lr=1e-3,\n",
    "    epochs=100,               \n",
    "    patience=15,\n",
    "    tol=1e-6,\n",
    "    status_print=10\n",
    "):\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)   \n",
    "\n",
    "    X_val_t, y_val_t, w_val_t = val_pack    # unpack\n",
    "  \n",
    "    best_val = float(\"inf\")                 # best validation loss; initialize at inf \n",
    "    best_state = None                       # Model weights when best val was achieved\n",
    "    bad = 0                                 # a counter for epochs without improvement in val \n",
    "\n",
    "    for ep in range(1, epochs + 1):          \n",
    "        # for each epoch complete pass\n",
    "        tr = train_one_epoch(model, \n",
    "                             inner_loader, \n",
    "                             opt, \n",
    "                             mode=mode)\n",
    "        \n",
    "        # for each epoch calculate eval loss \n",
    "        v  = eval_loss(model,                        \n",
    "                       X_val_t, \n",
    "                       y_val_t, \n",
    "                       w_val_t, \n",
    "                       mode=mode)\n",
    "\n",
    "\n",
    "        # Early stopping.                                        # if current loss < best_loss - tolerance\\\n",
    "        if v < best_val - tol:                                     # early stopping if no significant error \n",
    "            best_val = v                                            # update best val \n",
    "            best_state = copy.deepcopy(model.state_dict())          # to save the weights in the dataset\n",
    "            bad = 0                                                   # for each epoch count the bad \n",
    "        else:\n",
    "            bad += 1                                                # increment if no improvelement for the epoch\n",
    "\n",
    "        if ep == 1 or ep % status_print == 0:\n",
    "            print(f\"[{mode.upper()}] ep {ep:3d} | train {tr:.6f} | val {best_val:.6f} | bad {bad}/{patience}\")\n",
    "\n",
    "        # if bad epoch > patience stop \n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "    # restore the best model from the best epoch \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a89b8f",
   "metadata": {},
   "source": [
    "#### Test Set Tensor Preparation \n",
    "- Standardization: Test features are standardized using the mean (mu) and standard deviation (sd) computed on the training set only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87ee1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors \n",
    "def make_test_tensors(test_df, \n",
    "                      feature_cols, \n",
    "                      mu, \n",
    "                      sd, \n",
    "                      w_test, \n",
    "                      y_col=\"y_ck\"):\n",
    "    # raw arrays\n",
    "    X_test = test_df[feature_cols].to_numpy(np.float32)\n",
    "    y_test = test_df[y_col].to_numpy(np.float32)\n",
    "\n",
    "    # Standardize using taraing statistics - apply TRAIN-FIT scaler (mu, sd)\n",
    "    X_test_z = ((X_test - mu) / sd).astype(np.float32)\n",
    "\n",
    "    \n",
    "    # create tensors \n",
    "    X_test_t = torch.tensor(X_test_z, \n",
    "                            dtype=torch.float32, \n",
    "                            device=DEVICE)              # to have consistent output after division we convert again to float32\n",
    "    y_test_t = torch.tensor(y_test,   \n",
    "                            dtype=torch.float32, \n",
    "                            device=DEVICE)\n",
    "    w_test_t = torch.tensor(w_test,   \n",
    "                            dtype=torch.float32, \n",
    "                            device=DEVICE)\n",
    "    return X_test_t, y_test_t, w_test_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e823f",
   "metadata": {},
   "source": [
    "#### Metrics and  Evaluation \n",
    "The module below provides a the metrics to compare the three models (MSE, WMSE and MIX objective function) to understand the performace on the MSE, WMSE, RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf922ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict  \n",
    "# model eval mode on MSE, WMSE, MAE, RMSE\n",
    "@torch.no_grad()\n",
    "def predict(model, X_t):\n",
    "    model.eval()\n",
    "    return model(X_t).squeeze(-1).detach().cpu().numpy()      # no gradients to numpy array \n",
    "\n",
    "# METRICS \n",
    "# MSE \n",
    "def mse(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return float(np.mean((a - b) **2))\n",
    "\n",
    "# WMSE \n",
    "def wmse(a, b, w):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    w = np.asarray(w)\n",
    "    return float(np.sum(w * (a - b)**2) / np.sum(w))\n",
    "\n",
    "# Metrics \n",
    "# rmse \n",
    "def rmse(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return float(np.sqrt(np.mean((a - b) ** 2)))                # float gives better precision compared to np.float\n",
    "\n",
    "# mae \n",
    "def mae(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return float(np.mean(np.abs(a - b)))        \n",
    "\n",
    "# eval on the following metrics \n",
    "def eval_on_test(model, X_test_t, y_test_t, w_test_t):\n",
    "    yhat = predict(model, X_test_t)\n",
    "    ytrue = y_test_t.detach().cpu().numpy()\n",
    "\n",
    "    # unwighted results \n",
    "    results = {\n",
    "        \"mse\": mse(yhat, ytrue),\n",
    "        \"rmse\": rmse(yhat, ytrue),\n",
    "        \"mae\": mae(yhat, ytrue),\n",
    "        \"yhat_mean\": float(np.mean(yhat)),\n",
    "        \"y_mean\": float(np.mean(ytrue)),\n",
    "    }\n",
    "    \n",
    "    # weighted result addition \n",
    "    if w_test_t is not None:\n",
    "        w_test = w_test_t.detach().cpu().numpy()\n",
    "        results[\"wmse\"] = wmse(yhat, ytrue, w_test)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec5b83",
   "metadata": {},
   "source": [
    "### Model Experiment: MSE vs WMSE vs MIX Model \n",
    "This is a controlled experiment for comparing only the objective function on the same architecture and same dataset with same initialization. The only difference is the objective function. \n",
    "\n",
    "This block constitutes a clean ablation study where:\n",
    "- architecture and data are held constant,\n",
    "- initialization is identical across models,\n",
    "- training hyperparameters and early stopping rules are identical,\n",
    "- and the only treatment variable is the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MSE] ep   1 | train 0.010912 | val 0.084984 | bad 0/15\n",
      "[MSE] ep  10 | train 0.000935 | val 0.022316 | bad 0/15\n",
      "[MSE] ep  20 | train 0.000894 | val 0.022316 | bad 10/15\n",
      "[WMSE] ep   1 | train 0.009845 | val 0.017818 | bad 0/15\n",
      "[WMSE] ep  10 | train 0.000146 | val 0.004667 | bad 0/15\n",
      "[WMSE] ep  20 | train 0.000126 | val 0.003524 | bad 0/15\n",
      "[WMSE] ep  30 | train 0.000121 | val 0.003515 | bad 7/15\n",
      "[MIX] ep   1 | train 0.010548 | val 0.055954 | bad 0/15\n",
      "[MIX] ep  10 | train 0.000547 | val 0.015126 | bad 0/15\n",
      "[MIX] ep  20 | train 0.000514 | val 0.013146 | bad 7/15\n",
      "Best val mse: 0.02231566235423088\n",
      "Best val wmse: 0.0035145513247698545\n",
      "Best val mix: 0.013146189041435719\n",
      "TEST MSE model: {'mse': 0.07338245958089828, 'rmse': 0.2708919644355774, 'mae': 0.032291118055582047, 'yhat_mean': 0.3511728346347809, 'y_mean': 0.3792925179004669, 'wmse': 0.01044389232993126}\n",
      "TEST WMSE model: {'mse': 0.08704964816570282, 'rmse': 0.2950417697429657, 'mae': 0.03523220121860504, 'yhat_mean': 0.3530040681362152, 'y_mean': 0.3792925179004669, 'wmse': 0.012351471930742264}\n",
      "TEST MIX model: {'mse': 0.07659562677145004, 'rmse': 0.27675914764404297, 'mae': 0.03153848275542259, 'yhat_mean': 0.3538857698440552, 'y_mean': 0.3792925179004669, 'wmse': 0.010873016901314259}\n"
     ]
    }
   ],
   "source": [
    "# inner/val loaders and scaler \n",
    "inner_loader, val_pack, (mu_in, sd_in), (inner_mask, val_mask) = make_inner_val_loaders(\n",
    "                                                                        train_df,\n",
    "                                                                        feature_cols=FEATURES,\n",
    "                                                                        w_train_all=w_train,\n",
    "                                                                        date_col=\"snapshot_date\",\n",
    "                                                                        frac_train=0.9,\n",
    "                                                                        batch_size=BATCH\n",
    "                                                                    )\n",
    "\n",
    "# initialize models from identical weights - for comparision \n",
    "base = MLP(in_dim=len(FEATURES),                             # create a base state and use for MSE and G-WMSE Model and MIX Model  \n",
    "           hidden=(64,64), \n",
    "           dropout=0.0).to(DEVICE)\n",
    "base_state = copy.deepcopy(base.state_dict())\n",
    "\n",
    "\n",
    "# MSE-MODEL\n",
    "model_mse = MLP(in_dim=len(FEATURES),                 \n",
    "                hidden=(64,64),                             \n",
    "                dropout=0.0).to(DEVICE)\n",
    "model_mse.load_state_dict(base_state)\n",
    "\n",
    "\n",
    "# WMSE-MODEL\n",
    "model_wmse = MLP(in_dim=len(FEATURES), \n",
    "                 hidden=(64,64), \n",
    "                 dropout=0.0).to(DEVICE)\n",
    "model_wmse.load_state_dict(base_state)\n",
    "\n",
    "\n",
    "# MIX-MODEL\n",
    "model_mix = MLP(in_dim=len(FEATURES), \n",
    "                hidden=(64,64), \n",
    "                dropout=0.0).to(DEVICE)\n",
    "model_mix.load_state_dict(base_state)\n",
    "\n",
    "\n",
    "# train all three (same data, same init)\n",
    "# mse loss\n",
    "model_mse,  best_val_mse  = model_train_ES(model_mse,  \n",
    "                                        inner_loader, \n",
    "                                        val_pack, \n",
    "                                        mode=\"mse\",\n",
    "                                        lr=1e-3, \n",
    "                                        epochs=300, \n",
    "                                        patience=15)\n",
    "\n",
    "\n",
    "# wmse loss\n",
    "model_wmse, best_val_wmse = model_train_ES(model_wmse, \n",
    "                                    inner_loader, \n",
    "                                    val_pack, \n",
    "                                    mode=\"wmse\",\n",
    "                                    lr=1e-3, \n",
    "                                    epochs=300, \n",
    "                                    patience=15)\n",
    "\n",
    "# mix loss\n",
    "model_mix, best_val_mix = model_train_ES(\n",
    "    model_mix,\n",
    "    inner_loader,\n",
    "    val_pack,\n",
    "    mode=\"mix\",\n",
    "    lr=1e-3,\n",
    "    epochs=300,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "print(\"Best val mse:\", best_val_mse)\n",
    "print(\"Best val wmse:\", best_val_wmse)\n",
    "print(\"Best val mix:\", best_val_mix)\n",
    "\n",
    "\n",
    "# test tensors using the SAME mu/sd (mu_in, sd_in)\n",
    "X_test_t, y_test_t, w_test_t = make_test_tensors(test_df, FEATURES, mu_in, sd_in, w_test, y_col=\"y_ck\")\n",
    "\n",
    "# evaluate on test\n",
    "print(\"TEST MSE model:\",  eval_on_test(model_mse,  X_test_t, y_test_t, w_test_t))\n",
    "print(\"TEST WMSE model:\", eval_on_test(model_wmse, X_test_t, y_test_t, w_test_t))\n",
    "print(\"TEST MIX model:\", eval_on_test(model_mix, X_test_t, y_test_t, w_test_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca5c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (281288, 27)\n"
     ]
    }
   ],
   "source": [
    "# Get predictions (already have this function)\n",
    "@torch.no_grad()\n",
    "def predict(model, X_t):\n",
    "    model.eval()\n",
    "    return model(X_t).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "# Predict with all three models\n",
    "pred_mse  = predict(model_mse,  X_test_t)      # mse \n",
    "pred_wmse = predict(model_wmse, X_test_t)       # G-Wmse \n",
    "pred_mix  = predict(model_mix,  X_test_t)        # mix\n",
    "\n",
    "y_true = y_test_t.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# test dataframe for analysis\n",
    "test_df['pred_mse'] = pred_mse\n",
    "test_df['pred_wmse'] = pred_wmse\n",
    "test_df['error_mse'] = np.abs(pred_mse - y_true)\n",
    "test_df['error_wmse'] = np.abs(pred_wmse - y_true)\n",
    "test_df['sq_error_mse'] = (pred_mse - y_true) ** 2\n",
    "test_df['sq_error_wmse'] = (pred_wmse - y_true) ** 2\n",
    "\n",
    "print(f\"Shapes: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749212e4",
   "metadata": {},
   "source": [
    "##### Checks: Negative Option Price\n",
    "Since negative option price is violation of the option pricing, we check the role of `softplus` in the output layer which allows for strict non-negative call option pricing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cdd9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE model: 0 negative predictions (0.00%)\n",
      "WMSE model: 0 negative predictions (0.00%)\n",
      "MIX model: 0 negative predictions (0.00%)\n"
     ]
    }
   ],
   "source": [
    "def neg_report(name, preds):\n",
    "    n_neg = int((preds < 0).sum())\n",
    "    print(f\"{name}: {n_neg:,} negative predictions ({100*n_neg/len(preds):.2f}%)\")\n",
    "\n",
    "neg_report(\"MSE model\",  pred_mse)\n",
    "neg_report(\"WMSE model\", pred_wmse)\n",
    "neg_report(\"MIX model\",  pred_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cd09e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: preds_test.csv\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "out = test_df.copy()\n",
    "\n",
    "out[\"pred_ck_mse\"]  = pred_mse\n",
    "out[\"pred_ck_wmse\"] = pred_wmse\n",
    "out[\"pred_ck_mix\"]  = pred_mix              # mix\n",
    "\n",
    "# errors\n",
    "out[\"err_mse\"]  = out[\"pred_ck_mse\"]  - y_true\n",
    "out[\"err_wmse\"] = out[\"pred_ck_wmse\"] - y_true\n",
    "out[\"err_mix\"]  = out[\"pred_ck_mix\"]  - y_true   # mix\n",
    "\n",
    "out[\"abs_err_mse\"]  = np.abs(out[\"err_mse\"])\n",
    "out[\"abs_err_wmse\"] = np.abs(out[\"err_wmse\"])\n",
    "out[\"abs_err_mix\"]  = np.abs(out[\"err_mix\"])     # mix\n",
    "\n",
    "out[\"sq_error_mse\"]  = out[\"err_mse\"]**2\n",
    "out[\"sq_error_wmse\"] = out[\"err_wmse\"]**2\n",
    "out[\"sq_error_mix\"]  = out[\"err_mix\"]**2         # mix\n",
    "\n",
    "out.to_csv(\"preds_test.csv\", index=False)\n",
    "print(\"Saved: preds_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87d8838f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test weighted MSE metric:\n",
      "  MSE-model : 0.01044389232993126\n",
      "  WMSE-model: 0.012351471930742264\n",
      "  MIX-model : 0.010873016901314259\n"
     ]
    }
   ],
   "source": [
    "def wmse_metric(y, yhat, w):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat); w = np.asarray(w)\n",
    "    return float(np.sum(w * (y - yhat)**2) / (np.sum(w) + 1e-12))\n",
    "\n",
    "w_test_raw = gaussian_weights(test_df, lam=LAMBDA, beta=BETA, normalize_mean=False)\n",
    "\n",
    "print(\"\\nTest weighted MSE metric:\")\n",
    "print(\"  MSE-model :\",  wmse_metric(y_true, pred_mse,  w_test_raw))\n",
    "print(\"  WMSE-model:\",  wmse_metric(y_true, pred_wmse, w_test_raw))\n",
    "print(\"  MIX-model :\",  wmse_metric(y_true, pred_mix,  w_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f958eb",
   "metadata": {},
   "source": [
    "We analyzes model performance across moneyness regimes, with a specific focus on the at-the-money (ATM) region, which is the most liquid and economically relevant part of the option surface.\n",
    "\n",
    "The thresholds are chosen to:\n",
    "\n",
    "- separate deep out-of-the-money and deep in-the-money tails,\n",
    "- isolate a narrow ATM band around zero where pricing sensitivity (gamma) is highest,\n",
    "- align with common practitioner heuristics rather than arbitrary percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d69262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ATM REGION (M +- 0.05) Result:\n",
      "Samples: 70,193\n",
      "RMSE (MSE):  0.007673\n",
      "RMSE (WMSE): 0.002564  -  Improvement vs MSE: +66.6%\n",
      "RMSE (MIX):  0.004777   -  Improvement vs MSE: +37.7%\n"
     ]
    }
   ],
   "source": [
    "def categorize_moneyness(M):\n",
    "    if M < -0.5: return \"Deep OTM\"\n",
    "    elif M < -0.05: return \"OTM\"\n",
    "    elif M <= 0.05: return \"ATM\"\n",
    "    elif M <= 0.5: return \"ITM\"\n",
    "    else: return \"Deep ITM\"\n",
    "\n",
    "out[\"moneyness_region\"] = out[\"M\"].apply(categorize_moneyness)\n",
    "out[\"tradeable\"] = (out[\"y_ck\"] >= 0.001) & (out[\"y_ck\"] < 1.0)\n",
    "\n",
    "atm_mask = (out[\"M\"].abs() < 0.05)\n",
    "\n",
    "def atm_report(label, sq_err_col):\n",
    "    rmse_val = float(np.sqrt(out.loc[atm_mask, sq_err_col].mean()))\n",
    "    return rmse_val\n",
    "\n",
    "atm_rmse_mse  = atm_report(\"MSE\",  \"sq_error_mse\")\n",
    "atm_rmse_wmse = atm_report(\"WMSE\", \"sq_error_wmse\")\n",
    "atm_rmse_mix  = atm_report(\"MIX\",  \"sq_error_mix\")  \n",
    "\n",
    "print(\"\\nATM REGION (M +- 0.05) Result:\")\n",
    "print(f\"Samples: {int(atm_mask.sum()):,}\")\n",
    "print(f\"RMSE (MSE):  {atm_rmse_mse:.6f}\")\n",
    "print(f\"RMSE (WMSE): {atm_rmse_wmse:.6f}  -  Improvement vs MSE: {100*(atm_rmse_mse-atm_rmse_wmse)/atm_rmse_mse:+.1f}%\")\n",
    "print(f\"RMSE (MIX):  {atm_rmse_mix:.6f}   -  Improvement vs MSE: {100*(atm_rmse_mse-atm_rmse_mix)/atm_rmse_mse:+.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50f9a746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE BY REGION:\n",
      "  Region      n  RMSE (MSE)  RMSE (WMSE)  RMSE (MIX)  WMSE vs MSE %  MIX vs MSE %\n",
      "Deep OTM   7323 0.000249118  0.000255923 0.000318158       -2.73195      -27.7138\n",
      "     OTM  68390  0.00296564   0.00312274  0.00199487       -5.29747       32.7339\n",
      "     ATM  70193  0.00767337   0.00256411  0.00477735        66.5842       37.7412\n",
      "     ITM 101152  0.00801004   0.00788688  0.00510647        1.53763       36.2491\n",
      "Deep ITM  34230    0.776337     0.845648    0.793284       -8.92804      -2.18297\n"
     ]
    }
   ],
   "source": [
    "def compare_by_region(df):\n",
    "    rows = []\n",
    "    for region in [\"Deep OTM\", \"OTM\", \"ATM\", \"ITM\", \"Deep ITM\"]:\n",
    "        mask = (df[\"moneyness_region\"] == region)\n",
    "        n = int(mask.sum())\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        rmse_mse  = float(np.sqrt(df.loc[mask, \"sq_error_mse\"].mean()))\n",
    "        rmse_wmse = float(np.sqrt(df.loc[mask, \"sq_error_wmse\"].mean()))\n",
    "        rmse_mix  = float(np.sqrt(df.loc[mask, \"sq_error_mix\"].mean()))\n",
    "\n",
    "        rows.append({\n",
    "            \"Region\": region,\n",
    "            \"n\": n,\n",
    "            \"RMSE (MSE)\":  rmse_mse,\n",
    "            \"RMSE (WMSE)\": rmse_wmse,\n",
    "            \"RMSE (MIX)\":  rmse_mix,\n",
    "            \"WMSE vs MSE %\": 100.0 * (rmse_mse - rmse_wmse) / (rmse_mse + 1e-12),\n",
    "            \"MIX vs MSE %\":  100.0 * (rmse_mse - rmse_mix)  / (rmse_mse + 1e-12),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"PERFORMANCE BY REGION:\")\n",
    "reg_tab = compare_by_region(out)\n",
    "print(reg_tab.to_string(index=False, float_format=lambda x: f\"{x:.6g}\"))\n",
    "out.to_csv(\"predictions_with_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "16bd6c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region     n  MSE (MSE-model)  MSE (WMSE-model)  MSE (MIX-model)  WMSE vs MSE %  MIX vs MSE %\n",
      "Deep OTM  7298      6.22722e-08       6.57211e-08      1.01571e-07       -5.53836      -63.1072\n",
      "     OTM 62250      8.57759e-06       1.06981e-05       4.1091e-06        -24.721       52.0949\n",
      "     ATM 59642      5.07015e-05       5.90725e-06      1.84795e-05         88.349       63.5524\n",
      "     ITM 93189      6.60768e-05       6.58098e-05      2.74021e-05       0.404029         58.53\n",
      "Deep ITM 33643         0.607587          0.721492         0.634784       -18.7471       -4.4763\n"
     ]
    }
   ],
   "source": [
    "def compare_mse_by_region(df):\n",
    "    rows = []\n",
    "    for region in [\"Deep OTM\", \"OTM\", \"ATM\", \"ITM\", \"Deep ITM\"]:\n",
    "        mask = (df[\"moneyness_region\"] == region)\n",
    "        n = int(mask.sum())\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        y = df.loc[mask, \"y_ck\"].to_numpy()\n",
    "\n",
    "        mse_mse  = mse(df.loc[mask, \"pred_ck_mse\"].to_numpy(),  y)\n",
    "        mse_wmse = mse(df.loc[mask, \"pred_ck_wmse\"].to_numpy(), y)\n",
    "        mse_mix  = mse(df.loc[mask, \"pred_ck_mix\"].to_numpy(),  y)\n",
    "\n",
    "        rows.append({\n",
    "            \"Region\": region,\n",
    "            \"n\": n,\n",
    "            \"MSE (MSE-model)\":  mse_mse,\n",
    "            \"MSE (WMSE-model)\": mse_wmse,\n",
    "            \"MSE (MIX-model)\":  mse_mix,\n",
    "            \"WMSE vs MSE %\": 100.0 * (mse_mse - mse_wmse) / (mse_mse + 1e-12),\n",
    "            \"MIX vs MSE %\":  100.0 * (mse_mse - mse_mix)  / (mse_mse + 1e-12),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "mse_reg_tab = compare_mse_by_region(out)\n",
    "print(mse_reg_tab.to_string(index=False, float_format=lambda x: f\"{x:.6g}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e5167b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region     n  MAE (MSE-model)  MAE (WMSE-model)  MAE (MIX-model)  WMSE vs MSE %  MIX vs MSE %\n",
      "Deep OTM  7298      0.000172356        0.00018561      0.000221565       -7.68992      -28.5513\n",
      "     OTM 62250         0.002115        0.00175056       0.00136783        17.2313       35.3272\n",
      "     ATM 59642       0.00620547        0.00187335       0.00350259        69.8114       43.5564\n",
      "     ITM 93189       0.00573153        0.00547202        0.0024451        4.52776       57.3395\n",
      "Deep ITM 33643         0.231721          0.268218         0.242812       -15.7507      -4.78652\n"
     ]
    }
   ],
   "source": [
    "def compare_mae_by_region(df):\n",
    "    rows = []\n",
    "    for region in [\"Deep OTM\", \"OTM\", \"ATM\", \"ITM\", \"Deep ITM\"]:\n",
    "        mask = (df[\"moneyness_region\"] == region)\n",
    "        n = int(mask.sum())\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        y = df.loc[mask, \"y_ck\"].to_numpy()\n",
    "\n",
    "        mae_mse  = mae(df.loc[mask, \"pred_ck_mse\"].to_numpy(),  y)\n",
    "        mae_wmse = mae(df.loc[mask, \"pred_ck_wmse\"].to_numpy(), y)\n",
    "        mae_mix  = mae(df.loc[mask, \"pred_ck_mix\"].to_numpy(),  y)\n",
    "\n",
    "        rows.append({\n",
    "            \"Region\": region,\n",
    "            \"n\": n,\n",
    "            \"MAE (MSE-model)\":  mae_mse,\n",
    "            \"MAE (WMSE-model)\": mae_wmse,\n",
    "            \"MAE (MIX-model)\":  mae_mix,\n",
    "            \"WMSE vs MSE %\": 100.0 * (mae_mse - mae_wmse) / (mae_mse + 1e-12),\n",
    "            \"MIX vs MSE %\":  100.0 * (mae_mse - mae_mix)  / (mae_mse + 1e-12),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "mae_reg_tab = compare_mae_by_region(out)\n",
    "print(mae_reg_tab.to_string(index=False, float_format=lambda x: f\"{x:.6g}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL TEST FIT:\n",
      "                 mse     rmse       mae  RMSE_impr_%_vs_MSE  MAE_impr_%_vs_MSE  yhat_mean   y_mean\n",
      "MSE-model  0.0733825 0.270892 0.0322911                   0                  0   0.351173 0.379293\n",
      "WMSE-model 0.0870496 0.295042 0.0352322            -8.91492           -9.10803   0.353004 0.379293\n",
      "MIX-model  0.0765956 0.276759 0.0315385            -2.16587            2.33078   0.353886 0.379293\n"
     ]
    }
   ],
   "source": [
    "def overall_metrics(y, yhat):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    mse  = float(np.mean((y - yhat)**2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae  = float(np.mean(np.abs(y - yhat)))\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"yhat_mean\": float(np.mean(yhat)), \"y_mean\": float(np.mean(y))}\n",
    "\n",
    "m_mse  = overall_metrics(y_true, pred_mse)\n",
    "m_wmse = overall_metrics(y_true, pred_wmse)\n",
    "m_mix  = overall_metrics(y_true, pred_mix)\n",
    "\n",
    "summary = pd.DataFrame([m_mse, m_wmse, m_mix], index=[\"MSE-model\", \"WMSE-model\", \"MIX-model\"])\n",
    "summary[\"RMSE_impr_%_vs_MSE\"] = 100.0 * (summary.loc[\"MSE-model\",\"rmse\"] - summary[\"rmse\"]) / (summary.loc[\"MSE-model\",\"rmse\"] + 1e-12)\n",
    "summary[\"MAE_impr_%_vs_MSE\"]  = 100.0 * (summary.loc[\"MSE-model\",\"mae\"]  - summary[\"mae\"])  / (summary.loc[\"MSE-model\",\"mae\"]  + 1e-12)\n",
    "\n",
    "print(\"\\nOVERALL TEST FIT:\")\n",
    "print(summary[[\"mse\",\"rmse\",\"mae\",\"RMSE_impr_%_vs_MSE\",\"MAE_impr_%_vs_MSE\",\"yhat_mean\",\"y_mean\"]]\n",
    "      .to_string(float_format=lambda x: f\"{x:.6g}\"))\n",
    "summary.to_csv(\"overall_fit_3way.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0d9f4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x420 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.5, 4.2))\n",
    "\n",
    "plt.scatter(out[\"M\"], out[\"err_mse\"],  s=6, alpha=0.22, label=\"MSE\")\n",
    "plt.scatter(out[\"M\"], out[\"err_wmse\"], s=6, alpha=0.22, label=\"WMSE\")\n",
    "plt.scatter(out[\"M\"], out[\"err_mix\"],  s=6, alpha=0.22, label=f\"MIX (={ALPHA})\")\n",
    "\n",
    "plt.axhline(0, color=\"k\", lw=1)\n",
    "plt.axvline(0, color=\"k\", lw=1)\n",
    "\n",
    "plt.xlabel(\"Log-Moneyness M = log(S/K)\")\n",
    "plt.ylabel(\"Prediction Error (C/K)\")\n",
    "plt.title(\"Prediction Error vs Log-Moneyness (Test Set)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baede444",
   "metadata": {},
   "source": [
    "### Model Robustness Check \n",
    "no-arb diagnostic is checked on the test dataset, thus out of sample check. **This only include the tradable universe** which is T > 0.05 years, ultra-short dated options are highly sensetive to marketmicrostucture noise, wide bid-ask spread and discontinious pay-off behaviour near expiry. The stratification is done based on the moneyness where liquid universe is (M <= 0.2) which includes out-of-money, at-the-money and slightly in-the-money options, the deep-in-the-money options M (Log (S/K)) > 0.2 ~ 1.22 or 22% ITM  where the intrincic value dominates. \n",
    "\n",
    "#### No arb constrain \n",
    "For european call option, no-arb requires the option price to satisfy the lower bound \n",
    "\n",
    "$$C \\geq max(Se^{-qT} - K^{-rt}, 0).........(1)$$\n",
    "\n",
    "\n",
    "meaning the option must be worth atleast its discounted intrinsic value, otherwise, a static arb opportunity exist. Prices are evaluated in normalized in C/K form as above.  \n",
    "\n",
    "A lower-bound violation is recorded when the predicted normalized price falls below the theoretical bound by more than a numerical tolerance eps = 0.001, corresponding to $0.10 per $100 strike.This tolerance accounts for floating-point precision and is well within typical bid-ask spreads for SPY options. Market mid-prices are evaluated against the same no-arbitrage criterion and serve as a baseline. Due to **discretization** i.e. there can be small violation arising due to tick-size and use of mid quotes rather than executable price. Also microstructure effects (stale quotes, bid-ask spread due to liquidity), even observed market prices may exhibit occasional bound violations, providing a realistic benchmark for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563ffb6",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f10cf",
   "metadata": {},
   "source": [
    "### Pointwise (single-contract) No-Arb Checks \n",
    "There conditions must hold true for each options independently.\n",
    "1. non-negativity $$C \\geq 0$$\n",
    "2. American upper bound $$C \\leq Se^{qT}$$\n",
    "3. European Lower Bound (Diagnosis)\n",
    "    $$C \\geq max(Se^{qT} - Ke^{-rT}, 0)$$\n",
    "4. American Lower bound \n",
    "    $$C \\geq max(S - K, 0)$$\n",
    "\n",
    ">considering flat r = 0.035 and constant dividend rate of q = 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "28185a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.05  # in dollar tolearance \n",
    "r = 0.035\n",
    "q = 0.015 \n",
    "\n",
    "df = out.copy()\n",
    "# maturity filter \n",
    "df = df[df[\"t_ann\"] > 1/365].copy()\n",
    "\n",
    "# c/k to C dollar \n",
    "df[\"C_mkt\"] = df[\"mid\"]\n",
    "df[\"C_mse\"] = df[\"pred_ck_mse\"] * df[\"strike\"] \n",
    "df[\"C_wmse\"] = df[\"pred_ck_wmse\"] * df[\"strike\"]\n",
    "df[\"C_mix\"] = df[\"pred_ck_mix\"] * df[\"strike\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b1c79b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointwise bounds for Call option:\n",
    "# non-negativity -> C >= 0\n",
    "# upper bound European C <= Se^{qT}\n",
    "# lower bound C >= max(Se^{qT} - Ke^{-rT}, 0)\n",
    "# american lower bound for immediate exc: C >= max(S - K, 0)\n",
    "\n",
    "\n",
    "S = df[\"spot_price\"].astype(float)\n",
    "K = df[\"strike\"].astype(float)\n",
    "T = df[\"t_ann\"].astype(float)\n",
    "\n",
    "# american immediate exercise \n",
    "df[\"LB_am\"] = (S - K).clip(lower=0.0)\n",
    "\n",
    "# european lower bound with r and div yield q \n",
    "df[\"LB_eu\"] = (S * np.exp(-q*T) - K * np.exp(-r*T)).clip(lower=0.0)\n",
    " \n",
    "# upper bound \n",
    "df[\"UB_eu\"] = S*np.exp(-q*T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2c100880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>N</th>\n",
       "      <th>Non_Neg_Viol%</th>\n",
       "      <th>LB_Am_Viol(%)</th>\n",
       "      <th>LB_Eu_Viol(%)</th>\n",
       "      <th>Upper_Bound_Viol(%)</th>\n",
       "      <th>eps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Market</td>\n",
       "      <td>256022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569873</td>\n",
       "      <td>0.859301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSE</td>\n",
       "      <td>256022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.854059</td>\n",
       "      <td>13.309013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WMSE</td>\n",
       "      <td>256022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.507902</td>\n",
       "      <td>3.306356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIX</td>\n",
       "      <td>256022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.945497</td>\n",
       "      <td>7.465374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model       N  Non_Neg_Viol%  LB_Am_Viol(%)  LB_Eu_Viol(%)  \\\n",
       "0  Market  256022            0.0       0.569873       0.859301   \n",
       "1     MSE  256022            0.0      11.854059      13.309013   \n",
       "2    WMSE  256022            0.0       3.507902       3.306356   \n",
       "3     MIX  256022            0.0       6.945497       7.465374   \n",
       "\n",
       "   Upper_Bound_Viol(%)   eps  \n",
       "0                  0.0  0.05  \n",
       "1                  0.0  0.05  \n",
       "2                  0.0  0.05  \n",
       "3                  0.0  0.05  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_cols = {\n",
    "    \"Market\": \"C_mkt\",\n",
    "    \"MSE\":    \"C_mse\",\n",
    "    \"WMSE\":   \"C_wmse\",\n",
    "    \"MIX\":    \"C_mix\",\n",
    "}\n",
    "\n",
    "bounds_rows = []\n",
    "for name, col in price_cols.items():\n",
    "    C = df[col].astype(float)\n",
    "\n",
    "    # create. bool mask for violations \n",
    "    # Non-negativity: C >= 0, \n",
    "    v_nn = C < (0.0 - eps)           \n",
    "\n",
    "    # American LB: C >= LB_am\n",
    "    v_lb_am = C < (df[\"LB_am\"] - eps)\n",
    "\n",
    "    # European LB: C >= LB_eu\n",
    "    v_lb_eu = C < (df[\"LB_eu\"] - eps)\n",
    "\n",
    "    # Upper bound: C <= UB_eu\n",
    "    v_ub = C > (df[\"UB_eu\"] + eps)\n",
    "\n",
    "    # for each row we calcualte the \n",
    "    bounds_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"N\": len(df),\n",
    "        \"Non_Neg_Viol%\": 100.0 * v_nn.mean(),\n",
    "        \"LB_Am_Viol(%)\": 100.0 * v_lb_am.mean(),\n",
    "        \"LB_Eu_Viol(%)\": 100.0 * v_lb_eu.mean(),\n",
    "        \"Upper_Bound_Viol(%)\": 100.0 * v_ub.mean(),\n",
    "        \"eps\": eps\n",
    "    })\n",
    "\n",
    "bounds_report = pd.DataFrame(bounds_rows)\n",
    "bounds_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338b3fd",
   "metadata": {},
   "source": [
    "#### Vertical Arbitrage \n",
    "check monotonicity and convexity in each option chain (`snapshot_date`,  `expiration`) over strike \n",
    "1. Monotonicity in Strike - Vertical Spread Arb i.e. Call price must be non-increasing in strike \n",
    "$$C(K_1, T) \\geq C(K_2, T)$$\n",
    "\n",
    "2. Convexity in Maturity \n",
    "For Convexity in Maturity we avoid the butterfly arb, that is the cost of middle strike must be less than or eqaul to the weighted avg of outer strikes \n",
    "$$C_2 \\leq w_1*C_1 + w_2 * C_1 $$\n",
    "where: \n",
    "- $w_1 = \\frac{K_3 - K_2}{ K_3 - K_1}$\n",
    "- $w_2 = \\frac{K_2 - K_1}{ K_3 - K_1}$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\frac{\\partial^2 C}{\\partial K^2} \\ge 0$$\n",
    "\n",
    "\n",
    "- $\\text{slope}_i = \\frac{C_i - C_{i-1}}{K_i - K_{i-1}}$\n",
    "- $\\text{slope}_i - \\text{slope}_{i-1} \\ge 0$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e3251119",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"snapshot_date\", \"expiration\"]\n",
    "df = df.sort_values(group_cols + [\"strike\"]).copy()\n",
    "\n",
    "# precompute delta K \n",
    "df[\"dk\"] = df.groupby(group_cols)[\"strike\"].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "063391f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>N_contracts</th>\n",
       "      <th>N_groups</th>\n",
       "      <th>StrikeMonoViol%_avgGroup</th>\n",
       "      <th>StrikeConvexViol%_avgGroup</th>\n",
       "      <th>eps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Market</td>\n",
       "      <td>256022</td>\n",
       "      <td>1555</td>\n",
       "      <td>0.009795</td>\n",
       "      <td>5.976682</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSE</td>\n",
       "      <td>256022</td>\n",
       "      <td>1555</td>\n",
       "      <td>1.338013</td>\n",
       "      <td>2.180758</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WMSE</td>\n",
       "      <td>256022</td>\n",
       "      <td>1555</td>\n",
       "      <td>1.454091</td>\n",
       "      <td>3.054741</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIX</td>\n",
       "      <td>256022</td>\n",
       "      <td>1555</td>\n",
       "      <td>1.394541</td>\n",
       "      <td>2.394437</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  N_contracts  N_groups  StrikeMonoViol%_avgGroup  \\\n",
       "0  Market       256022      1555                  0.009795   \n",
       "1     MSE       256022      1555                  1.338013   \n",
       "2    WMSE       256022      1555                  1.454091   \n",
       "3     MIX       256022      1555                  1.394541   \n",
       "\n",
       "   StrikeConvexViol%_avgGroup   eps  \n",
       "0                    5.976682  0.05  \n",
       "1                    2.180758  0.05  \n",
       "2                    3.054741  0.05  \n",
       "3                    2.394437  0.05  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertical_rows = []\n",
    "\n",
    "for name, col in price_cols.items():\n",
    "    \n",
    "    # price for the given model \n",
    "    df[f\"C_{name}\"] = df[col].astype(float)\n",
    "    \n",
    "    # Monotonicity in C(K) must be non-increasing in K \n",
    "    # C_{i+1} - C_{i} <= eps \n",
    "    df[f\"dC_{name}\"] = df.groupby(group_cols)[f\"C_{name}\"].diff()\n",
    "    df[f\"monotone_vio_{name}\"] = df[f\"dC_{name}\"] > eps \n",
    "    \n",
    "    # convexity of C(K) in K (no butterfly arb) i.e. slopes are non-decreasing in strike   \n",
    "    # slope_i = (C_{i} - C_{i - 1}) / (K_i - K_{i - 1}) >= - eps_slope \n",
    "    df[f\"slope_{name}\"] =  df[f\"dC_{name}\"] /  df[\"dk\"] \n",
    "    \n",
    "    # for each chain \n",
    "    df[f\"dslope_{name}\"] = df.groupby(group_cols)[f\"slope_{name}\"].diff()\n",
    "    \n",
    "    #  median change in k per chain TO calculate eps_slope for the chain \n",
    "    median_dK = df.groupby(group_cols)[\"dk\"].transform(\"median\")\n",
    "    df[f\"eps_slope_{name}\"] = eps / median_dK\n",
    "    df[f\"conv_viol_{name}\"] = df[f\"dslope_{name}\"] < (-df[f\"eps_slope_{name}\"])\n",
    "    \n",
    "    \n",
    "    # average accross the chains \n",
    "    chain_mono = df.groupby(group_cols)[f\"monotone_vio_{name}\"].mean()\n",
    "    chain_convx = df.groupby(group_cols)[f\"conv_viol_{name}\"].mean()\n",
    "    \n",
    "    vertical_rows.append({\n",
    "        \"Model\": name, \n",
    "        \"N_contracts\": len(df),\n",
    "        \"N_groups\": int(chain_mono.shape[0]),\n",
    "        \"StrikeMonoViol%_avgGroup\": 100.0 * chain_mono.mean(),\n",
    "        \"StrikeConvexViol%_avgGroup\": 100.0 * chain_convx.mean(),\n",
    "        \"eps\":eps\n",
    "    })\n",
    "    \n",
    "vertical_rep = pd.DataFrame(vertical_rows)\n",
    "vertical_rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec06f36",
   "metadata": {},
   "source": [
    "All the model perform worst in Montonocity in strike as the are not trained across the chain in K. However, the model has smooth in Convexity as compared to the Strike in Convexity. MSE performs best because this might act as a low pass filter for the wiggle in the second order convexity. The market violations are caused by the microstrcture noise in the bids."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
